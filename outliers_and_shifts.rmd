---
title: 'Automated outlier and structural shift detection'
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_notebook:
    df_print: paged
    toc: yes
    toc_depth: 5
always_allow_html: yes    
    
editor_options: 
  chunk_output_type: inline
---

<!-- comments -->
<!-- -->

# Introduction

This program automates identification of level shifts in the DOH DSRIP data. It uses arima-based detection methods commonly used in econometrics.

It uses monthly PPS data that were current as of late December 2019.

The program examines approximately 1,400 PPS-measure combinations. Where data allow, it determines whether there are any level shifts and where the shifts are. It summarizes the number of shifts by month, by measure, and by PPS.

The results could be used to identify which combinations of PPS's and measures are suitable for interrupted time series analysis, and which are not.

<!-- 
# Setup code - run all of these chunks
This section, run silently, defines options, global variables, libraries, and selected functions.
-->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)
```


```{r system_specific_values, include=FALSE}
# ddir <- "C:/Users/donbo/Dropbox/DSRIP TSA_CA Team Info/chris_workflow/data/"
team_dir <- "C:/Users/donbo/Dropbox/DSRIP TSA_CA Team Info/"

chris_source_dir <- paste0(team_dir, "CMA Files/dsrip/chris_workflow/data/") # not used now

don_dir <- paste0(team_dir, "Don's Files/data/don/")
chris_dir <- paste0(team_dir, "Don's Files/data/chris_frozen/")

```


```{r globals, include=FALSE}
dsrip_start <- as.Date("2015-07-01")
```


```{r libraries, include=FALSE}
library("magrittr")
library("plyr") # needed for ldply; must be loaded BEFORE dplyr
library("tidyverse")
options(tibble.print_max = 60, tibble.print_min = 60) # if more than 60 rows, print 60 - enough for states
# ggplot2 tibble tidyr readr purrr dplyr stringr forcats

library("scales")
library("hms") # hms, for times
library("lubridate") # lubridate, for date/times
library("vctrs")

library("grDevices")
library("knitr")
library("kableExtra")

library("btools")

library("DT")

library("zoo") # for rollapply

library("broom") # for automating the cleanup of complex output
library("tsoutliers")

library("janitor")

```


```{r functions_general, include=FALSE}
# general functions that might be used anywhere in the program

ns <- function(df) {names(df) %>% sort}


msr_name <- function(msr_id, measures_df=measures) {
  # return the measure name MSR_RESULT_NAME for a given measure id MSR_RESULT_ID
  # requires as input a dataframe that has both MSR_RESULT_ID and MSR_RESULT_NAME
  tibble(MSR_RESULT_ID=msr_id) %>%
    left_join(measures_df) %>%
    .$MSR_RESULT_NAME
}


pps_wrap <- function(PPS_ID, PPS_NAME, maxlen=100, wrap=25) {
  name <- str_sub(PPS_NAME, 1, maxlen)
  paste0(str_pad(PPS_ID, 2, pad="0"), ": ", name) %>% # zero-pad for better sorting
    str_wrap(wrap)
}


```


# Get combined PPS/Statewide data
<!-- -->
Get previously created data:

* Monthly PPS data constructed from Chris's data as of late December 2019. I used long-format data and added statewide averages. I called the statewide data PPS 99 so that it would appear last in plots sorted by PPS id.
* Tables with information on measures and on PPS names.

The data were created from Chris's original data in [this program](https://github.com/donboyd5/DSRIP/blob/master/its_on_DSRIP_data.rmd), in the "ONETIME data prep" section.


```{r getdata, include=FALSE}
# get previously created data
ppsdf <- readRDS(paste0(don_dir, "ppsall_ma.rds")) # monthly pps and statewide data, long
measures <- readRDS(paste0(don_dir, "measures.rds"))
ppsnames <- readRDS(paste0(don_dir, "ppsnames.rds"))
measures

# to be on the safe side make sure ppsdf is ungrouped and sorted
ppsdf <- ppsdf %>%
  ungroup %>%
  mutate(idname=pps_wrap(PPS_ID, PPS_NAME, maxlen=1000, wrap=1000)) %>% # unwrapped concatenation; 100, 25 defaults
  arrange(PPS_ID, MSR_RESULT_ID, PER_END_DT) 

```


# Data check before analysis
Two data checks below:

* How many PPS-measure combinations have NA values and should we drop combinations that have NAs?
* How many observations are left in the remaining PPS-measure combinations and should we drop combinations with fewer than a certain number of observations?


```{r include=FALSE, eval=FALSE}
# examine NA values

ns(ppsdf)
summary(ppsdf)
ppsdf %>% select(PPS_ID, MSR_RESULT_ID) %>% unique %>% nrow # 1,398 unique PPS-measure combinations
# 113 out of 61245 obs have NA MSR_RESULT
ppsdf %>% filter(is.na(MSR_RESULT)) %>% count(MSR_RESULT_ID)
ppsdf %>% filter(is.na(MSR_RESULT)) %>% count(idname)
ppsdf %>% filter(is.na(MSR_RESULT)) %>% count(MSR_RESULT_ID, idname) %>% arrange(-n)
# 21 PPS-measure combinations will be lost

# look at the worst
ppsdf %>% 
  filter(MSR_RESULT_ID=="PDI15RES", PPS_ID==45) %>% 
  select(MSR_RESULT_ID, idname, PER_END_DT, MSR_NUM, MSR_DEN, MSR_RESULT)
# looks like they could have instead chosen to zero-out MSR_RESULT, but not enough people to use this anyway

# look at a case with only one NA
ppsdf %>% 
  filter(MSR_RESULT_ID=="PQI15RES", PPS_ID==32) %>% 
  select(MSR_RESULT_ID, idname, PER_END_DT, MSR_NUM, MSR_DEN, MSR_RESULT)
# same thing - looks like the numerator was zero and they could have zeroed this out
# but there are too few peole to attribute any meaning to this one also

# conclusion: let's drop all combinations that have any NA's

```


## PPS-measure combinations that have NA observations

Delete all PPS-measure combinations that have NAs

```{r}
# show key result of the NA analysis
ppsdf %>% 
  filter(is.na(MSR_RESULT)) %>%
  mutate(idname=pps_wrap(PPS_ID, PPS_NAME, maxlen=1000, wrap=1000)) %>%
  group_by(MSR_RESULT_ID, MSR_RESULT_NAME, idname) %>%
  summarise(n=n()) %>% 
  arrange(-n) %>%
  adorn_totals() %>%
  kable(caption="# of NA observations by measure and PPS", format="html") %>%
  kable_styling()

```


## Number of observations in remaining combinations

The table below shows counts of PPS-measure combinations by the number of observations.

```{r include=TRUE, eval=TRUE}
# how many observations do we have in each PPS-measure group if we drop all combinations that have any NAs (per above)?
check1 <- ppsdf %>%
  group_by(PPS_ID, MSR_RESULT_ID) %>%
  mutate(has_nas=any(is.na(MSR_RESULT))) %>%
  ungroup %>%
  filter(!has_nas)
# verify that we lost 21 combinations
fulln <- ppsdf %>% select(PPS_ID, MSR_RESULT_ID) %>% unique %>% nrow 
checkn <- check1 %>% select(PPS_ID, MSR_RESULT_ID) %>% unique %>% nrow 
# fulln - checkn # 21
# rm(fulln, checkn)

# now count obs
check1 %>%
  group_by(PPS_ID, MSR_RESULT_ID) %>%
  summarise(nobs=n()) %>%
  group_by(nobs) %>%
  summarise(ncombinations=n()) %>%
  arrange(-nobs) %>%
  adorn_totals() %>%
  kable(caption="Number of PPS-measure combinations, by number of good observations available",
        format="html") %>%
  kable_styling()
  
```

Two questions: 

1. Which month tends to be missing if we only have 56 observations instead of full 57?
2. (To Do): do any of the combinations with fewer observations look salvageable?


```{r}
# get the dates we expect in the full data
full_dates <- check1 %>%
  group_by(PPS_ID, MSR_RESULT_ID) %>%
  mutate(nobs=n()) %>%
  ungroup %>%
  filter(nobs==57) %>%
  .$PER_END_DT %>%
  unique

obs56 <- check1 %>%
  group_by(PPS_ID, MSR_RESULT_ID) %>%
  mutate(nobs=n()) %>%
  ungroup %>%
  filter(nobs==56) %>%
  select(PPS_ID, idname, MSR_RESULT_ID, MSR_RESULT_NAME, PER_END_DT)

missing_dates <-
  obs56 %>%
  group_by(PPS_ID, idname, MSR_RESULT_ID, MSR_RESULT_NAME) %>%
  summarise(missdate=setdiff(full_dates, PER_END_DT) %>% as.Date) %>%
  ungroup

count(missing_dates, missdate)

```


For this analysis, for now we'll drop all that have fewer than 56 observations and we'll drop the combination that has 56 observations and is missing 2018-01-31.


```{r include=FALSE}
# create a clean data file that has the bad observations removed
baddates <- function(dates){
  # flag whether a vector of dates is only 56 obs long and includes
  # 2014-06-30, which means it is missing an interior date
  ndates <- length(dates)
  bad <- (ndates==56) & (as.Date("2014-06-30") %in% dates)
  bad56 <- rep(bad, ndates)
  bad56
}

drop_obs <- ppsdf %>%
  group_by(PPS_ID, MSR_RESULT_ID) %>%
  mutate(has_nas = any(is.na(MSR_RESULT)),
         under56 = n()<56,
         bad56 = baddates(PER_END_DT)) %>%
  ungroup

drop_obs %>%
  summarise_at(vars(has_nas, under56, bad56), list(~ sum(.)))

drop_groups <- drop_obs %>%
  group_by(PPS_ID, MSR_RESULT_ID) %>%
  summarise(has_nas=sum(has_nas), under56=sum(any(under56)), bad56=sum(any(bad56))) %>%
  mutate(drop=has_nas | under56 | bad56) %>%
  ungroup

drop_groups %>%
  summarise_at(vars(has_nas, under56, bad56, drop), list(~ sum(.)))
# we'll be dropping 535 of the 1398 groups we started with

ppsdf_clean <- ppsdf %>%
  right_join(drop_groups %>%
               filter(drop==FALSE) %>%
               select(PPS_ID, MSR_RESULT_ID),
            by=c("PPS_ID", "MSR_RESULT_ID"))
# verify
ppsdf %>% select(PPS_ID, MSR_RESULT_ID) %>% unique %>% nrow  # 1398
ppsdf_clean %>% select(PPS_ID, MSR_RESULT_ID) %>% unique %>% nrow  # 863
# good  

```


# Level-shift analysis

```{r include=FALSE}
# functions to identify outliers in a vector and return a data frame of outlier locations

get_level_shift <- function(vec, crit.val=3, delta.val=0.7){
  # This is the simple version without error trapping. It works on most data. It is not used in this program
  # because too many DSRIP series cause errors. Instead I used an error-trapping version.
  
  make_empty_row <- function(vec){
    # define 1-row data frame to return if we have bad data or bad results
    outdf <- tibble(nobs=length(vec), type=factor(NA_character_), ind=NA_integer_, coefhat=NA_real_, tstat=NA_real_)
    outdf
  }
  
  if(length(vec) < 36) return(make_empty_row(vec)) # too few observations, won't be necessary if we drop bad groups
  
  tsobj <- ts(vec, frequency=1) # the tso function requires a time-series object
  
  inner.loop <- 10
  outer.loop <- 6
  
  # get each of the 4 kinds of outliers separately, rather than all at once - I have found this to work well
  out <- tsoutliers::tso(tsobj, types = "LS", maxit.iloop = inner.loop,
                         cval = crit.val,
                         tsmethod="arima",
                         args.tsmethod = list(order=c(0, 1, 1)))

  if(nrow(out$outliers)==0) return(make_empty_row(vec))
  
  # we only get here if we have an acceptable result
  outdf <- out$outliers %>% 
    mutate(nobs=length(vec)) %>%
    select(nobs, type, ind, coefhat, tstat) %>% 
    as_tibble
  
  return(outdf)
}


get_level_shift_trapping <- function(vec, crit.val=3, delta.val=0.7){
  # This version has more error trapping so that it is unlikely to bomb when used with hundreds of
  # time series, some of which may be very poorly behaved.
  
  # function to identify different level shifts
  #   vec is a numeric vector (sorted by time sequence)
  #   the return will be outdf, a data frame with information on outliers in the data, including their positions
  
  # this is a robust version that looks for several kinds of problems that could occur in the data:
  #  - an input vector that has too few values to run through the  outliers routine
  #  - an error that causes the arima optimization routine
  #     to fail (I am not sure I know when this occurs, but it is rare)
  #  - a problem in which the arima optimzation routine runs but does not return a data frame of outlier locations
  #     presumably something was not right in the data
  #  - zero ouliers found - not really a problem, we just need to exit gracefully
  
  # Kinds of outliers
  # An Additive Outlier (AO) represents an isolated spike
  # A Level Shift (LS) represents an abrupt change in the mean level
  # A Temporary Change (TC) is a change in the level of the series that decreases exponentially
  
  # focus on finding points that meet level-shift criteria
  
  # the lower crit.val is, the more sensitive this is to outliers (the more likely to find them)
  # the default is NULL and is calculated by tso to be about 3 for our data
  
  # lower delta.val also tends to find more outliers; 0.7 is default
  
  make_empty_row <- function(vec){
    # define 1-row data frame to return if we have bad data or bad results
    outdf <- tibble(nobs=length(vec), type=factor(NA_character_), ind=NA_integer_, coefhat=NA_real_, tstat=NA_real_)
    outdf
  }
  
  # I'm not sure what to set as the minimum vector length but for now I use 3 years
  if(length(vec) < 36) return(make_empty_row(vec)) # too few observations
  
  # now we're ready to start
  tsobj <- ts(vec, frequency=1) # the tso function requires a time-series object
  
  inner.loop <- 10
  outer.loop <- 6
  
  # we have to put the tso function into a function that calls it and looks for an error
  runtso <- function(tsobj) {
    out <- tryCatch(
      {
        # run the function
        tsoutliers::tso(tsobj, types = "LS", maxit.iloop = inner.loop,
                        cval = crit.val,
                        tsmethod="arima",
                        args.tsmethod = list(order=c(0, 1, 1))) # not sure this is the best order but it works well
        },
      # now we can decide what to do if we cautht an error
      error=function(e) {
        return(make_empty_row(vec)) # exit with proper output
        }
      )
    # if we get here the arima optimization ran but that doesn't necessarily mean the results are good
    # we'll check that further below
    return(out)
  }
  outlist <- runtso(tsobj)
  
  # the "outliers" data frame may not exist in outlist so we have to check for that - not sure when this will happen
  if(!exists("outliers", where=outlist)) return(make_empty_row(vec))
  
  # if we get here, arima ran AND we have an outliers data frame, but it may not have any rows
  if(nrow(outlist$outliers)==0) return(make_empty_row(vec))
  
  # we only get here if we have an acceptable result so we add nobs and return the outliers data frame
  outdf <- outlist$outliers %>% 
    mutate(nobs=length(vec)) %>%
    select(nobs, type, ind, coefhat, tstat) %>% 
    as_tibble
  
  return(outdf)
}

```


<!-- -->
<!-- 
Ok to uncomment this heading.
# Test the function on some known situations - several smooth, several with breaks
-->

```{r eval=FALSE}
# create test data sample of vectors with different characteristics, based upon in spection of plots
defs <- tribble(
  ~vectype, ~MSR_RESULT_ID, ~PPS_ID,
  "smooth", "AAPC20RES", 25,
  "smooth", "CAPC12MRES", 14,
  "smooth", "PDI14RES", 36,
  "break", "AMRRES", 40, # 1 break
  "break", "PPVRES", 33,
  "break-sharpfall", "CAPC12RES", 36,
  "outliers2", "AMMACUTRES", 48,
  "break", "AAPC20RES", 16,
  "break", "AAPC20RES", 19,
  "break", "AAPC45RES", 34,
  "break", "AMRRES", 52
) %>%
  mutate(groupnum=row_number()) %>%
  left_join(measures %>% select(MSR_RESULT_ID, MSR_RESULT_NAME)) %>%
  left_join(ppsnames)
defs
ns(defs)

test_data <- ppsdf %>% 
  select(MSR_RESULT_ID, PPS_ID, PER_END_DT, time, PER_END_DT, MSR_RESULT) %>%
  right_join(defs, by=c("MSR_RESULT_ID", "PPS_ID"))
ns(test_data)
glimpse(test_data)

fls <- function(df, crit.val) {get_level_shift_trapping(df$MSR_RESULT, crit.val = crit.val)}

d <- test_data %>%
  nest(data=c(PER_END_DT, time, MSR_RESULT)) %>%
  mutate(out=map(data, fls, crit.val=4.5))
d

outdf <- d %>%
  unnest(out, keep_empty = TRUE)
outdf

vlines <- outdf %>%
  filter(!is.na(ind)) %>%
  select(groupnum, MSR_RESULT_ID, PPS_ID, idname, ind) %>%
  mutate(time=ind - 13)
vlines

brks <- c(seq(0, -30, -3) %>% rev, seq(3, 60, 3))
p <- test_data %>%
  # filter(groupnum %in% c(4, 5, 8)) %>%
  ggplot(aes(time, MSR_RESULT)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks=brks) +
  geom_vline(xintercept = 1, colour="blue", linetype="solid") +
  geom_vline(xintercept = 12, colour="blue", linetype="dashed") +
  geom_vline(aes(xintercept=time), colour="red", data=vlines) +
  theme(axis.text.x=element_text(angle=90, vjust=0.5)) +
  theme_bw() +
  facet_wrap(~MSR_RESULT_ID + idname, ncol=3, scales = "free") +
  theme(legend.position = "none")
p
glimpse(test_data)

```



# Analyze level shifts
The first step in this section is to run the level-shift detection routine on all combinations of PPS's and measures, and to save the results.

The second step is to summarize results by month, by measure, and by PPS.

```{r get_shifts, include=FALSE}
ns(ppsdf_clean)

# define function that gets and sends a vector to our level-shift identification function
fls <- function(df, crit.val) {get_level_shift_trapping(df$MSR_RESULT, crit.val = crit.val)}

# call the function separately on each subgroup of the data and keep the results in the data frame "shifts"
a <- proc.time()
shifts <- ppsdf_clean %>% 
  select(MSR_RESULT_ID, PPS_ID, PER_END_DT, rownum, time, MSR_RESULT, PPS_NAME, MSR_RESULT_NAME, idname) %>%
  nest(data=c(PER_END_DT, rownum, time, MSR_RESULT)) %>%
  mutate(out=map(data, fls, crit.val=4.25)) # outlier flagging is very sensitive to crit.val
# may throw warnings that it set some residuals to zero
b <- proc.time()
b - a
glimpse(shifts)

shifts_long <- shifts %>%
  # keep result records for all PPS-MSR cells, even NA records for cells that had no level shifts
  unnest(out, keep_empty = TRUE) %>% 
  # bring in the data and time variables associated with shift points
  left_join(ppsdf_clean %>% select(PPS_ID, MSR_RESULT_ID, ind=rownum, PER_END_DT, time),
            by=c("PPS_ID", "MSR_RESULT_ID", "ind"))
glimpse(shifts_long)

# quick summary of those by number of observations
d <- shifts_long %>%
  group_by(MSR_RESULT_ID, MSR_RESULT_NAME) %>%
  summarise(nobs=mean(nobs))


count(shifts_long, MSR_RESULT_ID)

count(shifts_long %>% filter(!is.na(PER_END_DT)), PER_END_DT) %>% arrange(desc(n))
count(shifts_long %>% filter(!is.na(PER_END_DT)), month(PER_END_DT)) %>% arrange(desc(n))

```


```{r}
shifts_long %>%
  group_by(PPS_ID, MSR_RESULT_ID) %>%
  summarise(nbreaks=sum(!is.na(ind))) %>%
  ungroup %>%
  summarise(ngroups=n(), n_withbreaks=sum(nbreaks>0)) %>%
  mutate(n_withoutbreaks=ngroups - n_withbreaks,
         pct_withbreaks=n_withbreaks / ngroups * 100)
```


## Number of level shifts by month
```{r results='asis'}
shifts_long %>%
  filter(!is.na(ind)) %>%
  mutate(month=month(PER_END_DT),
         month=factor(month, levels=1:12, labels=month.name)) %>%
  group_by(month) %>%
  summarise(n=n()) %>%
  arrange(-n) %>%
  adorn_totals() %>%
  mutate(pctshare=n / max(n) * 100) %>%
  kable(digits=1, format="html", caption="Number of level shifts by month") %>%
  kable_styling()

```


```{r include=FALSE}
# summarize the data -- for each MSR-PPS cell, get # of breaks found
glimpse(shifts_long)

cell_breaks <- shifts_long %>%
  group_by(MSR_RESULT_ID, MSR_RESULT_NAME, PPS_ID, PPS_NAME, idname) %>%
  # do NOT delete NA before summarizing - we want to keep a group that only has an NA record, and
  # therefore has 0 breaks - we'll count it as 0 breaks in the code below
  summarise(nbreaks=sum(!is.na(ind)))
# cell_breaks %>% select(-nbreaks) %>% anyDuplicated() # good, no duplicates - we're only counting unique MSR-PPS cells

```


## Measures with the greatest and least number of PPS's that have breaks
### Measures ranked by number of PPS's with level shifts
```{r meas_pct_table}
# for each MSR get % of PPS that had breaks
meas_pct <- cell_breaks %>%
  group_by(MSR_RESULT_ID, MSR_RESULT_NAME) %>%
  summarise(npps=n(), n_with_breaks=sum(nbreaks>0)) %>%
  ungroup %>%
  mutate(pct_with_breaks=n_with_breaks / npps * 100) %>%
  arrange(-n_with_breaks)

meas_pct %>%
  datatable() %>%
  formatRound(columns=c('pct_with_breaks'), digits=1)

```

### Facet plots of measures with most number of PPS's that have level shifts
```{r mplot, fig.height=12, fig.width=12}
# calculate for each MSR-PPS where vertical lines indicating breaks should be drawn
vlines <- shifts_long %>%
  filter(!is.na(ind)) %>%
  select(MSR_RESULT_ID, MSR_RESULT_NAME, PPS_ID, idname, ind) %>%
  mutate(time=ind - 13)
# vlines

mplot <- function(msrid) {
  xlabels <- c(seq(0, -30, -3) %>% rev, seq(3, 60, 3))
  subt <- "Blue lines mark DSRIP start (time=1) and moving-average phase in (time=12). Red lines mark break points. Pink points mark MY4."
  
  p <- ppsdf_clean %>%
    filter(MSR_RESULT_ID==msrid) %>%
    ggplot(aes(time, MSR_RESULT)) +
    geom_line() +
    geom_point() +
    scale_x_continuous(breaks=xlabels) +
    geom_vline(xintercept = 1, colour="blue", linetype="solid") +
    geom_vline(xintercept = 12, colour="blue", linetype="dashed") +
    geom_vline(aes(xintercept=time), colour="red", data=vlines %>% filter(MSR_RESULT_ID==msrid)) +
    geom_point(aes(time, MSR_RESULT), colour="pink", data=. %>% filter(my4==1)) +
    theme(axis.text.x=element_text(angle=90, vjust=0.5)) +  
    theme_bw() +
    facet_wrap(~idname, ncol=5, scales = "free") +
    theme(legend.position = "none") +
    ggtitle(paste0(msrid, ": ", msr_name(msrid)),
            subtitle=subt)
  return(p)
}

for(msrid in meas_pct$MSR_RESULT_ID[1:5]) {
  print(mplot(msrid))
}

```

### Facet plots of measures with fewest PPS's that have level shifts
```{r mplot2, fig.height=12, fig.width=12}
# let's limit this to measures where we have a lot of observations
bottom <- (nrow(meas_pct) - 4):nrow(meas_pct)
for(msrid in meas_pct$MSR_RESULT_ID[bottom]) {
  print(mplot(msrid))
}

```

## PPS's with the greatest and least number of measures that have breaks
### PPS's ranked by number of measures with level shifts
```{r pps_pct_table}
# for each MSR get % of PPS that had breaks
pps_pct <- cell_breaks %>%
  group_by(PPS_ID, PPS_NAME, idname) %>%
  summarise(nmeas=n(), n_with_breaks=sum(nbreaks>0)) %>%
  ungroup %>%
  mutate(pct_with_breaks=n_with_breaks / nmeas * 100) %>%
  arrange(-n_with_breaks)

pps_pct %>%
  datatable() %>%
  formatRound(columns=c('pct_with_breaks'), digits=1)

```

### Facet plots of PPS's with most number of measures that have level shifts
```{r pplot, fig.height=12, fig.width=12}
# calculate for each MSR-PPS where vertical lines indicating breaks should be drawn
vlines <- shifts_long %>%
  filter(!is.na(ind)) %>%
  select(MSR_RESULT_ID, MSR_RESULT_NAME, PPS_ID, idname, ind) %>%
  mutate(time=ind - 13)
# vlines

pplot <- function(ppsid) {
  xlabels <- c(seq(0, -30, -3) %>% rev, seq(3, 60, 3))
  subt <- "Blue lines mark DSRIP start (time=1) and moving-average phase in (time=12). Red lines mark break points. Pink points mark MY4."
  plotname <- ppsdf_clean %>% filter(PPS_ID==ppsid);   plotname <- plotname$idname[1]
  measids <- meas_pct$MSR_RESULT_ID[1:25] # grab measures with the greatest number of breaks (trusting the sort)
  
  # ppsid <- 99
  p <- ppsdf_clean %>%
    filter(PPS_ID==ppsid, MSR_RESULT_ID %in% measids) %>%
    ggplot(aes(time, MSR_RESULT)) +
    geom_line() +
    geom_point() +
    scale_x_continuous(breaks=xlabels) +
    geom_vline(xintercept = 1, colour="blue", linetype="solid") +
    geom_vline(xintercept = 12, colour="blue", linetype="dashed") +
    geom_vline(aes(xintercept=time), colour="red", data=vlines %>% filter(PPS_ID==ppsid)) +
    geom_point(aes(time, MSR_RESULT), colour="pink", data=. %>% filter(my4==1)) +
    theme(axis.text.x=element_text(angle=90, vjust=0.5)) +  
    theme_bw() +
    facet_wrap(~MSR_RESULT_ID, ncol=5, scales = "free") +
    theme(legend.position = "none") +
    ggtitle(plotname,
            subtitle=subt)
  return(p)
}

for(ppsid in pps_pct$PPS_ID[1:5]) {
  print(pplot(ppsid))
}


```


### Facet plots of PPS's with fewest measures that have level shifts
```{r pplot2, fig.height=12, fig.width=12}
# calculate for each MSR-PPS where vertical lines indicating breaks should be drawn
bottom <- (nrow(pps_pct)-4):nrow(pps_pct)
for(ppsid in pps_pct$PPS_ID[bottom]) {
  print(pplot(ppsid))
}

```

