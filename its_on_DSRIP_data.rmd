---
title: 'Preliminary ITS analysis'
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_notebook:
    df_print: paged
    toc: yes
    toc_depth: 5
always_allow_html: yes    
    
editor_options: 
  chunk_output_type: inline
---


# Setup code - run all of these chunks
<!-- setup chunk -->
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)
```


```{r system_specific_values, include=FALSE}
# ddir <- "C:/Users/donbo/Dropbox/DSRIP TSA_CA Team Info/chris_workflow/data/"
team_dir <- "C:/Users/donbo/Dropbox/DSRIP TSA_CA Team Info/"

chris_source_dir <- paste0(team_dir, "CMA Files/dsrip/chris_workflow/data/") # not used now

don_dir <- paste0(team_dir, "Don's Files/data/don/")
chris_dir <- paste0(team_dir, "Don's Files/data/chris_frozen/")

```


```{r globals, include=FALSE}
m_long <- "V_mapp_pps_msr_mo_long.csv"
a_long <- "v_mapp_pps_msr_ann_long.csv"

# ppsnet_long <- "pps_net_df.csv" # looks like it could have useful data

dsrip_start <- as.Date("2015-07-01")

```


```{r libraries, include=FALSE}
library("magrittr")
library("plyr") # needed for ldply; must be loaded BEFORE dplyr
library("tidyverse")
options(tibble.print_max = 60, tibble.print_min = 60) # if more than 60 rows, print 60 - enough for states
# ggplot2 tibble tidyr readr purrr dplyr stringr forcats

library("scales")
library("hms") # hms, for times
library("lubridate") # lubridate, for date/times
library("vctrs")

library("grDevices")
library("knitr")
library("kableExtra")

library("btools")

library("DT")

library("zoo") # for rollapply

library("broom")
library("forecast") # for auto.arima
library("lmtest")
library("tsoutliers")

# devtools::install_github("business-science/anomalize")
library("anomalize")


```


```{r functions, include=FALSE}
ns <- function(df) {names(df) %>% sort}

ma <- function(x, period) {
  # create trailing moving average of x of length period
  zoo::rollapply(x, period, function(x) mean(x, na.rm=TRUE), fill=NA, align="right")
}

se <- function(model) {sqrt(diag(vcov(model)))}

msr_name <- function(msr_id, measures_df=measures) {
  # return the measure name MSR_RESULT_NAME for a given measure id MSR_RESULT_ID
  # requires as input a dataframe that has both MSR_RESULT_ID and MSR_RESULT_NAME
  tibble(MSR_RESULT_ID=msr_id) %>%
    left_join(measures_df) %>%
    .$MSR_RESULT_NAME
}

msr_improve <- function(msr_id, measures_df=measures) {
  # return the measure improve direction for a given measure id MSR_RESULT_ID
  # requires as input a dataframe that has both MSR_RESULT_ID and MSR_RESULT_NAME
  tibble(MSR_RESULT_ID=msr_id) %>%
    left_join(measures_df) %>%
    .$improve
}

pps_wrap <- function(PPS_ID, PPS_NAME, maxlen=100, wrap=25) {
  name <- str_sub(PPS_NAME, 1, maxlen)
  paste0(str_pad(PPS_ID, 2, pad="0"), ": ", name) %>% # zero-pad for better sorting
    str_wrap(wrap)
}


```


# ONETIME data prep

DO NOT RUN THESE CHUNKS UNLESS USING NEW/IMPROVED DATA.

The code in this section is designed to be run only once, to create and save convenient versions of data files. To run any of this code again, either step through it interactively or change the chunk headers to "eval=TRUE".

Note: The pps-level data files used here originally came from the Dropbox folder:

    DSRIP TSA_CA Team Info/chris_workflow/data/
    
On approximately December 20, 2019 I copied the files from that location to:

    DSRIP TSA_CA Team Info/Don's Files/data/chris_frozen/
    
so that they would be "frozen" until I copied updated versions. In the analysis below I use these frozen versions, although it is possible to rerun the analysis with new files by pointing to Chris's source directory rather than the frozen directory.    


## Extract metadata from Chris's project-level data

Create a file with information on each measure:

- Label
- Units (how it is calculated)
- Whether improvement requires an increase or decrease

```{r ONETIME_metadata, eval=FALSE}
# the rds file v_mapp_pps_msr_ann_long-CRTH9H2.rds appears to have a lot of useful metadata
# there do not appear to be other files that are as close to raw data as this

# There appear to be useful date variables that may tell us the vintage of the data used in constructing measures.
# There is also informatoin on goals and targets.
# And information on the units that is helpful in calculating measures.

chrisdf <- readRDS(paste0(chris_dir, "v_mapp_pps_msr_ann_long-CRTH9H2.rds"))
glimpse(chrisdf)
count(chrisdf, PPS_ID, PPS_NAME)

# create a master data frame with information on measures and their units
measures1 <- chrisdf %>%
  filter(PPS_ID != 51) %>%
  group_by(MSR_RESULT_ID, MSR_RESULT_NAME, UNIT_LBL) %>%
  summarise(npps=length(unique(PPS_ID))) %>%
  ungroup
# 179 measures
count(measures1, UNIT_LBL)
count(measures1, npps)

measures1 %>% filter(is.na(UNIT_LBL)) # does not look like we will need any of these

# add a variable "improve" to signify whether improvement is measured by increase or decrease
measures <- measures1 %>% 
  arrange(MSR_RESULT_ID) %>%
  mutate(improve=case_when(
    # specific measures first, to override text parsing
    
    # measures that improve when they increase
    # NOTE: AMRRES higher means more use of controller meds, less of rescue meds
    MSR_RESULT_ID %in% c("ADVQUITRES", "AMRRES", "COMMRES", "CUSTRES",
                         "EZINSTRES", paste0("HI", c(1, 2, 4)),
                         "OSRES", "RATERES") ~ "increase",  
    
    # measures that improve when they decrease
    MSR_RESULT_ID %in% c("DD1RES",
                         paste0("HIV", 1:3),
                         "HPAINRES", "LSRDSRES",
                         "PAINCRES",
                         "PDI90RES", "PDI91RES", "PDI92RES",
                         "PQI9RES", "PQI13RES", "PQI16RES",
                         "PQI90RES", "PQI90V2RES", "PQI90V3RES",
                         "PQI91RES", 
                         "PQI92RES", "PQI92V2RES", "PQI92V3RES",
                         paste0("Obese", 1:2),
                         paste0("Preg", 1:5),
                         paste0("Prem", 1:7),
                         "Smok") ~ "decrease",
    
    # the following improve when they increase
    str_detect(MSR_RESULT_NAME, coll("access", ignore_case=TRUE)) ~ "increase",
    str_detect(MSR_RESULT_NAME, coll("adher", ignore_case=TRUE)) ~ "increase",
    str_detect(MSR_RESULT_NAME, coll("care", ignore_case=TRUE)) ~ "increase",
    str_detect(MSR_RESULT_NAME, coll("controlling", ignore_case=TRUE)) ~ "increase",
    str_detect(MSR_RESULT_NAME, coll("discuss", ignore_case=TRUE)) ~ "increase",
    str_detect(MSR_RESULT_NAME, coll("engagement", ignore_case=TRUE)) ~ "increase",
    str_detect(MSR_RESULT_NAME, coll("follow", ignore_case=TRUE)) ~ "increase",
    str_detect(MSR_RESULT_NAME, coll("initiation", ignore_case=TRUE)) ~ "increase",
    str_detect(MSR_RESULT_NAME, coll("improve", ignore_case=TRUE)) ~ "increase",
    str_detect(MSR_RESULT_NAME, coll("literacy", ignore_case=TRUE)) ~ "increase",
    str_detect(MSR_RESULT_NAME, coll("manage", ignore_case=TRUE)) ~ "increase",
    str_detect(MSR_RESULT_NAME, coll("monitor", ignore_case=TRUE)) ~ "increase",
    str_detect(MSR_RESULT_NAME, coll("offered", ignore_case=TRUE)) ~ "increase",
    str_detect(MSR_RESULT_NAME, coll("receiv", ignore_case=TRUE)) ~ "increase",
    str_detect(MSR_RESULT_NAME, coll("screen", ignore_case=TRUE)) ~ "increase",
    str_detect(MSR_RESULT_NAME, coll("talk", ignore_case=TRUE)) ~ "increase",
    str_detect(MSR_RESULT_NAME, coll("use of", ignore_case=TRUE)) ~ "increase",
                           
    # the following improve when they decrease
    str_detect(MSR_RESULT_NAME, coll("adolescent pregnancy", ignore_case=TRUE)) ~ "decrease",
    str_detect(MSR_RESULT_NAME, coll("age-adjusted", ignore_case=TRUE)) ~ "decrease",
    str_detect(MSR_RESULT_NAME, coll("avoidable", ignore_case=TRUE)) ~ "decrease",
    str_detect(MSR_RESULT_NAME, coll("admission rate", ignore_case=TRUE)) ~ "decrease",
    str_detect(MSR_RESULT_NAME, coll("complication", ignore_case=TRUE)) ~ "decrease",
    str_detect(MSR_RESULT_NAME, coll("ED use", ignore_case=TRUE)) ~ "decrease",
    str_detect(MSR_RESULT_NAME, coll("emergency department visit", ignore_case=TRUE)) ~ "decrease",
    str_detect(MSR_RESULT_NAME, coll("mortality rate", ignore_case=TRUE)) ~ "decrease",
    str_detect(MSR_RESULT_NAME, coll("preventable", ignore_case=TRUE)) ~ "decrease",
    
    # all other
    TRUE ~ "unknown"))
count(measures, improve)
count(measures %>% filter(npps==25), improve) # 69 measures for all pps, where we know what improvement is

measures %>% filter(improve=="increase")
measures %>% filter(improve=="decrease")
measures %>% filter(improve=="unknown")

saveRDS(measures, paste0(don_dir, "measures.rds"))

# check
measures <- readRDS(paste0(don_dir, "measures.rds"))
glimpse(measures)
ht(measures)


#.. look at MSR_AS_OF_DT ----
# do as_of dates change within a measure-pps-project?
# mdates <- chris_m %>%
#   group_by(MSR_RESULT_ID, MSR_RESULT_NAME, DSRIP_PROJ_ID, DSRIP_PROJ_TITLE, PPS_ID, PPS_NAME) %>%
#   summarise(ndates=length(unique(MSR_AS_OF_DT)))
# from other analysis - the MSR_AS_OF_DT can change within a measure, project, pps, over time -- it looks like they must
# calculate a measure and then keep it for later dates

```


## Simple data checks and create a clean monthly file
```{r ONETIME_clean, eval=FALSE}
# read as character to look for trouble
ppsm <- read_csv(paste0(chris_dir, m_long), col_types = cols(.default= col_character()))
glimpse(ppsm)

count(ppsm, PPS_ID, PPS_NAME) # good, we don't have pps 51
# are there any measures not in our measures database?
setdiff(unique(ppsm$MSR_RESULT_ID), measures$MSR_RESULT_ID) # nope, none

# do some light data cleaning
ppsm2 <- ppsm %>%
  mutate(PPS_ID=as.integer(PPS_ID),
         PER_END_DT=as.Date(str_sub(PER_END_DT, 1, 10))) %>% # 2014-06-30 has time included, don't parse that
  mutate_at(vars(MSR_NUM, MSR_DEN, MSR_RESULT, PERFRM_GOAL), ~as.numeric(.))
summary(ppsm2)

# look at some missing values for the numerator
ppsm2 %>% filter(is.na(MSR_NUM))
# are they really missing? look at the character data
missids <- ppsm2 %>% filter(is.na(MSR_NUM)) %>% .[["X1"]]
ppsm %>% filter(X1 %in% missids) # yes, missing in the character data, also; I guess we have to accept it for now

# quick check: is MSR_RESULT approx equal to NUM / DEN scaled?
measure_check <- ppsm2 %>%
  left_join(measures %>% select(MSR_RESULT_ID, UNIT_LBL), by = "MSR_RESULT_ID") %>%
  mutate(denom=case_when(UNIT_LBL == "Per 1,000 Newborns" ~ MSR_DEN / 1000,
                         UNIT_LBL == "Per 100 Members" ~ MSR_DEN / 100,
                         UNIT_LBL == "Per 100,000 Members" ~ MSR_DEN / 100e3,
                         UNIT_LBL == "Percentage" ~ MSR_DEN / 100,
                         TRUE ~ 9e-99),
         msr_calc=MSR_NUM / denom,
         diff=msr_calc - MSR_RESULT) %>%
  filter(diff!=0)
quantile(measure_check$diff, na.rm=TRUE) # good, they match almost perfectly

```


## Create slimmed-down file with statewide records, as pps 99
```{r ONETIME_state, eval=FALSE}

state <- ppsm2 %>%
  group_by(MSR_RESULT_ID, PER_END_DT) %>% 
  summarise_at(vars(MSR_NUM, MSR_DEN), ~sum(., na.rm=TRUE)) %>%
  mutate(PPS_ID=99, PPS_NAME="Statewide")

# add the state total and calc measures
pps3 <- bind_rows(ppsm2, state) %>%
  left_join(measures %>% select(MSR_RESULT_ID, MSR_RESULT_NAME, UNIT_LBL, improve)) %>%
  mutate(denom=case_when(UNIT_LBL == "Per 1,000 Newborns" ~ MSR_DEN / 1000,
                         UNIT_LBL == "Per 100 Members" ~ MSR_DEN / 100,
                         UNIT_LBL == "Per 100,000 Members" ~ MSR_DEN / 100e3,
                         UNIT_LBL == "Percentage" ~ MSR_DEN / 100,
                         TRUE ~ 9e-99),
         msr_calc=MSR_NUM / denom,
         MSR_RESULT=ifelse(PPS_ID==99, msr_calc, MSR_RESULT),
         idname=paste0(str_pad(PPS_ID, 2, pad="0"), ": ", PPS_NAME) %>% str_wrap(25)) %>% # zero-pad for better sorting
  arrange(MSR_RESULT_ID, PPS_ID, PER_END_DT)
# tmp <- pps3 %>% filter(abs(MSR_RESULT - msr_calc) > 0.1) # good, no errors
glimpse(pps3)
summary(pps3)
ppsall <- pps3 %>% select(-c(X1, denom, msr_calc))
saveRDS(ppsall, paste0(don_dir, "ppsall.rds"))

ppsall <- readRDS(paste0(don_dir, "ppsall.rds"))
glimpse(ppsall)

```


## Add dsrip dummy, time relative to dsrip start, and moving averages 

```{r ONETIME_addma, eval=FALSE}
ppsall <- readRDS(paste0(don_dir, "ppsall.rds"))

# create moving average of time, dsrip, and interaction variables
# create a sequence of month end dates is first of each month minus 1 day
(end_dates <- seq(as.Date("2013-02-01"), length=96, by="1 month") - 1) # Jan 2013 - Dec 2020
ym <- function(date){paste(year(date), month(date))}

timedf <- tibble(PER_END_DT=end_dates) %>%
  mutate(rownum=row_number(),
         time = rownum - rownum[which(ym(PER_END_DT) == ym(dsrip_start))] + 1,
         dsrip=ifelse(PER_END_DT >= dsrip_start, 1, 0),
         interaction=time * dsrip,
         time_ma=ma(time, 12),
         dsrip_ma=ma(dsrip, 12),
         interaction_ma=ma(interaction, 12)) %>%
  # filter(!is.na(time_ma)) %>%
  filter(PER_END_DT >= "2014-06-30") %>%
  mutate(rownum=row_number()) # reset this to be 1 at dsrip_start
timedf
saveRDS(timedf, paste0(don_dir, "timedf.rds"))

ppsall_ma <- ppsall %>%
  left_join(timedf, by = "PER_END_DT")

glimpse(ppsall_ma)
count(ppsall_ma, MSR_RESULT_ID)
count(ppsall_ma, improve)
ppsall_ma %>% filter(improve=="unknown")
measures %>% filter(MSR_RESULT_ID=="LSRAPRES") # Antipsychotic Use in Persons with Dementia +/-

saveRDS(ppsall_ma, paste0(don_dir, "ppsall_ma.rds"))

```


# START OF ANALYSIS -- with combined PPS/Statewide data
```{r getdata, include=FALSE}
ppsdf <- readRDS(paste0(don_dir, "ppsall_ma.rds"))
measures <- readRDS(paste0(don_dir, "measures.rds"))
measures

```


## Exploration -- test out ITS regression, different methods

DO NOT RUN THIS SECTION UNLESS EXPLORING.

This section walks through a set of ITS regressions, but the results will not be shown unless you change the chunk header to "eval=TRUE". It starts with a simple OLS model that is tainted because it does not account for either the moving-average nature of the dependent variable or the moving-average nature of the error term. It moves through a series of ARIMA models, ending with the proper model that specifies both the data and the error term correctly.


```{r its_explore, eval=FALSE}
msrid <- "PPRRES"
ppsnum <- 27

msrid <- "AMRRES"
ppsnum <- 99

measures %>% filter(MSR_RESULT_ID==msrid)

df <- ppsdf %>%
  filter(MSR_RESULT_ID==msrid) %>%
  filter(PPS_ID==ppsnum)

df %>%
  select(PER_END_DT, time, time_ma, dsrip, dsrip_ma, interaction, interaction_ma)

df %>%
  select(time, time_ma, MSR_RESULT) %>%
  mutate(x=time_ma) %>%
  ggplot(aes(x, MSR_RESULT)) + 
  geom_line() + 
  geom_point() + 
  geom_vline(xintercept = 0) + 
  scale_x_continuous(breaks=seq(-24, 48, 2))


xvars <- as.matrix(df[, c("time", "dsrip", "interaction")])
xvars_ma <- as.matrix(df[, c("time_ma", "dsrip_ma", "interaction_ma")])


# build from simple lm model to the model above
# lm model, dsrip impact big and signif
lm1 <- lm(MSR_RESULT ~ time + dsrip + interaction, data=df)
summary(lm1)
# Coefficients:
#             Estimate Std. Error t value Pr(>|t|)    
# (Intercept)  647.174     10.881  59.479  < 2e-16 ***
# time          -2.167      1.539  -1.408    0.165    
# dsrip        -53.790     12.607  -4.267 8.22e-05 ***
# interaction          1.452      1.558   0.932    0.356 


# same model using arima call, same results, dsrip impact big and signif
mod1 <- arima(df$MSR_RESULT, xreg=xvars, order=c(0, 0, 0)) 
summary(mod1)
# Coefficients:
#       intercept     time     dsrip   interaction
#        647.1744  -2.1667  -53.7900  1.4524
# s.e.    10.4920   1.4838   12.1565  1.5027

# add ar1, dsrip smaller and not signif
mod1 <- arima(df$MSR_RESULT, xreg=xvars, order=c(1, 0, 0)) 
summary(mod1)
#          ar1  intercept     time     dsrip   interaction
#       0.7649   628.1793  -4.0510  -23.1627  2.9181
# s.e.  0.0866    16.2951   2.1741   14.3668  2.4234


# ar1 with ma variables, dsrip impact bigger but not signif
mod1 <- arima(df$MSR_RESULT, xreg=xvars_ma, order=c(1, 0, 0))
summary(mod1)
# Coefficients:
#          ar1  intercept  time_ma  dsrip_ma  interaction_ma
#       0.6810   620.1582  -3.3444  -50.8206    3.3376
# s.e.  0.0953    20.7246   1.7342   27.4244    1.7452

# auto.arima of proper data ar1, ar2 large prob not significant
mod1 <- auto.arima(df$MSR_RESULT, xreg=xvars_ma)
summary(mod1)
# Coefficients:
#          ar1      ar2  intercept  time_ma  dsrip_ma  interaction_ma
#       0.8379  -0.2338   620.4940  -3.3059  -52.6447    3.4013
# s.e.  0.1272   0.1295    18.2177   1.5831   24.2835    1.5628

# proper model ma11, neg but not signif
mod1 <- arima(df$MSR_RESULT, xreg=xvars_ma, order=c(0, 0, 11), fixed = c(rep(1, 11), NA, NA, NA, NA ))
summary(mod1)
# Coefficients:
#       intercept  time_ma  dsrip_ma  interaction_ma
#       592.4594  -6.4147  -32.1812    7.0997
# s.e.  77.9107   5.8765   98.9464    6.2526


Acf(residuals(mod1))
Pacf(residuals(mod1))

```


## ITS regression on multiple measures

The regressions below are based on the arima method that takes into consideration moving-average dependent variables and moving-average error terms as discussed [here](https://github.com/donboyd5/DSRIP/blob/master/appendix_its_ma.rmd).

Steps still to do:

*  assumption checks
*  robustness checks
*  one-sided ztests?
*  automatic outlier identification
*  automatic shift identification
   

```{r}
# ITS issues and assumptions
# - linear trend or transormation to linear
# - gradual introduction of intervention or multiple introductions
# - external time-varying events
# - autocorrelation
# - changing characteristics of population over time



```


```{r its_functions, include=FALSE}
# functions we'll use throughout the its analysis
ztest <- function(mod){
  ztest <- lmtest::coeftest(mod)
  ztdf <- ztest[, ] %>% 
    as.data.frame() %>%
    setNames(c("estimate", "std.error", "z.value", "p.value")) %>%
    mutate(term=rownames(ztest)) %>%
    as_tibble() %>%
    select(term, everything())
  ztdf
}


sig_type <- function(improve, shift.coeff, shift.pvalue, slope.coeff, slope.pvalue, pcut=.05) {
  # type of significance
  sig_type <- case_when(
    (improve == "increase") &
      (shift.coeff > 0) &
      (shift.pvalue <= pcut) &
      (slope.coeff > 0) &
      (slope.pvalue <= pcut) ~ "both",
      
    (improve == "increase") &
      (shift.coeff > 0) &
      (shift.pvalue <= pcut) ~ "shift",
    
    (improve == "increase") &
      (slope.coeff > 0) &
      (slope.pvalue <= pcut) ~ "slope",
    
    (improve == "decrease") &
      (shift.coeff < 0) &
      (shift.pvalue <= pcut) &
      (slope.coeff < 0) &
      (slope.pvalue <= pcut) ~ "both",
      
    (improve == "decrease") &
      (shift.coeff < 0) &
      (shift.pvalue <= pcut) ~ "shift",
    
    (improve == "decrease") &
      (slope.coeff < 0) &
      (slope.pvalue <= pcut) ~ "slope",
    
    TRUE ~ "none")
  
  return(sig_type)
}


get_sig <- function(results_df, shift, slope, pvalue.cut) {
  results_df <- results_df %>%
    mutate(sig_type=sig_type(improve, 
                             shift.coeff=.[[paste0(shift, ".coeff")]],
                             shift.pvalue=.[[paste0(shift, ".pvalue")]],
                             slope.coeff=.[[paste0(slope, ".coeff")]],
                             slope.pvalue=.[[paste0(slope, ".pvalue")]],
                             pcut=pvalue.cut),
           good=ifelse(sig_type %in% c("both", "shift", "slope"), 1, 0))
  return(results_df)
}


reg_results <- function(regressions) {
  # pass detailed regression results, return a list with coeffs, pvalues and summary results
  coeffs <- regressions %>% 
    unnest(tidied) %>%
    select(PPS_ID, MSR_RESULT_ID, term, estimate) %>%
    pivot_wider(names_from = term, values_from = estimate)

  pvalues <- regressions %>% 
    unnest(ztest) %>%
    select(PPS_ID, MSR_RESULT_ID, term, p.value) %>%
    pivot_wider(names_from = term, values_from = p.value)
  
  results <- regressions_mafixed %>%
    select(PPS_ID, PPS_NAME, MSR_RESULT_ID) %>%
    left_join(measures %>% select(MSR_RESULT_ID, MSR_RESULT_NAME, improve), by = "MSR_RESULT_ID") %>%
    left_join(coeffs, by = c("PPS_ID", "MSR_RESULT_ID")) %>%
    left_join(pvalues, by = c("PPS_ID", "MSR_RESULT_ID"), suffix = c(".coeff", ".pvalue"))

  
  reg_list <- list()
  reg_list$coeffs <- coeffs
  reg_list$pvalues <- pvalues
  reg_list$results <- results
  
  return(reg_list)
}

```


```{r its_data, include=FALSE}
# run the regressions and consolidate results

# Note: this uses the arima method for moving-average dependent variables, as discussed here:
#    https://github.com/donboyd5/DSRIP/blob/master/appendix_its_ma.rmd


# useful automation notes:
# https://cran.r-project.org/web/packages/broom/vignettes/broom.html
# https://cran.r-project.org/web/packages/broom/vignettes/broom_and_dplyr.html
# https://drsimonj.svbtle.com/running-a-model-on-separate-groups

# get a list of potential measures
msr_use <- measures %>%
  filter(improve != "unknown") %>% # npps==25, 
  filter(!str_detect(MSR_RESULT_ID, "PQI90"),
         !str_detect(MSR_RESULT_ID, "PQI92"))
msr_use
msr_ids <- msr_use$MSR_RESULT_ID

# before starting, get observation counts for these measures and pps's and drop any that aren't enough
obs_counts <- ppsdf %>%
  filter(MSR_RESULT_ID %in% msr_ids) %>%
  group_by(PPS_ID, MSR_RESULT_ID) %>%
  summarise(n=n(), n.notNA=sum(!is.na(MSR_RESULT))) %>%
  ungroup
obs_counts %>% filter(n.notNA < n)
obs_counts %>% filter(n == n.notNA, n==57) # ~ 500 regressions

regdata <- ppsdf %>%
  right_join(obs_counts %>% filter(n == n.notNA, n==57))

glimpse(regdata)
nregs <- regdata %>%
  group_by(PPS_ID, PPS_NAME, MSR_RESULT_ID) %>%
  summarise(n=n()) %>%
  ungroup
# a total of 605 regressions
count(nregs, PPS_ID, PPS_NAME)
count(nregs, MSR_RESULT_ID) %>% arrange(-n) # number of pps per measure

```



```{r its_arima_auto, include=FALSE}
a <- proc.time()
xvars <- c("time_ma", "dsrip_ma", "interaction_ma")
regressions_automa <- regdata %>%
  # additional filtering if desired:
  # filter(PPS_ID %in% (c(1:99)) %>%
  arrange(PPS_ID, MSR_RESULT_ID, time) %>%
  nest(data=-c(MSR_RESULT_ID, MSR_RESULT_NAME, PPS_ID, PPS_NAME)) %>%
    mutate(
    fit = map(data, ~ auto.arima(.$MSR_RESULT,
                                 xreg = as.matrix(.[, xvars]))),
    tidied = map(fit, tidy),
    glanced = map(fit, glance),
    ztest=map(fit, ztest))
b <- proc.time()
b - a # 150 seconds for 605 regressions
saveRDS(regressions_automa, here::here("results", "regressions_automa.rds"))

regressions_automa <- readRDS(here::here("results", "regressions_automa.rds"))
regressions_automa %>% ht

# rlist <- reg_results(regressions_automa)
# 
# results <- rlist$results
# glimpse(results)
# results <- get_sig(results, shift="dsrip_ma", slope="interaction_ma", .05)
# count(results, sig_type)
# mean(results$good)

```



```{r its_arima_mafixed, include=FALSE}
a <- proc.time()
xvars <- c("time_ma", "dsrip_ma", "interaction_ma")
regressions_mafixed <- regdata %>%
  # additional filtering if desired:
  # filter(PPS_ID %in% (c(1:99)) %>%
  arrange(PPS_ID, MSR_RESULT_ID, time) %>%
  nest(data=-c(MSR_RESULT_ID, MSR_RESULT_NAME, PPS_ID, PPS_NAME)) %>%
    mutate(
    fit = map(data, ~ arima(.$MSR_RESULT, 
                            xreg = as.matrix(.[, xvars]),
                            order=c(0, 0, 11),
                            fixed = c(rep(1, 11), NA, NA, NA, NA ))),
    tidied = map(fit, tidy),
    glanced = map(fit, glance),
    ztest=map(fit, ztest))
b <- proc.time()
b - a # 20 seconds for 605 regressions
saveRDS(regressions_mafixed, here::here("results", "regressions_mafixed.rds"))

regressions_mafixed <- readRDS(here::here("results", "regressions_mafixed.rds"))
regressions_mafixed %>% ht

```


## Summary of results
```{r include=FALSE}
regressions <- readRDS(here::here("results", "regressions_automa.rds"))
glimpse(regressions)

rlist <- reg_results(regressions)

results <- rlist$results
glimpse(results)

# significance pvalue for regressions -- define in only 1 place to avoid mistakes
pcut <- .01
# pcut <- .05
# pcut <- .10

results <- get_sig(results, shift="dsrip_ma", slope="interaction_ma", pvalue.cut=pcut)
count(results, sig_type)
mean(results$good)

# count the number of regressions by number of pps's involved
nregs <- results %>%
  group_by(MSR_RESULT_ID) %>%
  summarise(n=n()) %>%
  group_by(n) %>%
  summarise(nregressions=sum(n)) %>%
  select(npps=n, nregressions) %>% 
  arrange(-npps)

nregs

```



This program ran a total of `r nrow(results)` regressions, including `r length(unique(results$MSR_RESULT_ID))` measures, `r length(setdiff(unique(results$PPS_ID), 1))` PPS's, and the statewide weighted average. I only included combinations of measures and PPS's for which there were no missing observations.

Here is a table of the number of regressions by the number of PPS's included (counting the statewide average as a PPS):

```{r results='asis'}
nregs %>% janitor::adorn_totals() %>% kable()
```


Overall, DSRIP had a significant effect in `r sprintf("%2.1f", mean(results$good) * 100)`% of these regressions. I considered the DSRIP effect significant if the DSRIP intercept or DSRIP slope or both were significantly different from 0 at the `r sprintf("%0.2f", pcut)` level and the relevant DSRIP coefficient(s) had the appropriate sign for improvement in the measure.


## Tables of results
```{r include=TRUE, paged.print=TRUE}

h2 <- paste0("DSRIP slope or intercept was significant (", sprintf("%0.2f", pcut), " level) and measure was improving")

h1 <- "For each measure, percent of PPS's for which"
results %>%
  group_by(MSR_RESULT_ID, MSR_RESULT_NAME) %>%
  summarise(nregressions=n(), pct_signif=mean(good) * 100) %>%
  arrange(-pct_signif) %>%
  datatable(caption=htmltools::tags$caption(paste0(h1, " ", h2), style="text-align:left;")) %>%
  formatRound(columns=c('pct_signif'), digits=1)


h1 <- "For each PPS, percent of measures for which"
results %>%
  group_by(PPS_ID, PPS_NAME) %>%
  summarise(nregressions=n(), pct_signif=mean(good) * 100) %>%
  arrange(-pct_signif) %>%
  datatable(caption=htmltools::tags$caption(paste0(h1, " ", h2), style="text-align:left;")) %>%
  formatRound(columns=c('pct_signif'), digits=1)


```


## Facet plots for the 5 measures that had the greatest percentage of PPS's for which DSRIP was significant and indicated improvement
```{r meas_regplot, fig.height=12, fig.width=12}
# get the measures we want
mlist <- results %>%
  group_by(MSR_RESULT_ID, MSR_RESULT_NAME) %>%
  summarise(nregressions=n(), pct_signif=mean(good) * 100) %>%
  arrange(-pct_signif)

f <- function(meas) {
  pdata <- ppsdf %>%
    filter(MSR_RESULT_ID == meas) %>%
    left_join(results %>%
                select(PPS_ID, MSR_RESULT_ID, good) %>%
                mutate(good=factor(good))) %>%
    mutate(idname=pps_wrap(PPS_ID, PPS_NAME, maxlen=70, wrap=35),
           my4=ifelse(PER_END_DT >= "2017-07-01" &  PER_END_DT <= "2018-03-31", 1, 0) %>% as.integer) %>%
    arrange(PPS_ID, time)
  npanels <- length(unique(pdata$PPS_ID))
  ncolumns <- case_when(npanels <= 2 ~ 1,
                        npanels %in% 3:8 ~ 2,
                        npanels %in% 9:15 ~ 3,
                        npanels %in% 16:20 ~ 4,
                        TRUE ~ 5)
  
  p <- pdata %>%
  ggplot(aes(time, MSR_RESULT, colour=good)) +
  scale_colour_manual(values=c("black", "orange"), drop=FALSE) +
  geom_line() +
  geom_point() +
  geom_point(aes(time, MSR_RESULT), colour="pink", data=. %>% filter(my4==1)) +
  scale_x_continuous(breaks=brks) +
  geom_vline(xintercept = 1, colour="blue", linetype="solid") +
  geom_vline(xintercept = 12, colour="blue", linetype="dashed") +
  theme_bw() +
  facet_wrap(~idname, ncol=ncolumns, scales = "free") +
  theme(legend.position = "none") +
  theme(axis.text.x=element_text(angle=90, vjust=0.5)) +
  ggtitle(paste0(meas, ": ", msr_name(meas)),
          subtitle=paste0("Improvement requires ", msr_improve(meas), ".",
                          " PPS's with significant DSRIP effect (slope or intercept) at ",
                          sprintf("%.2f", pcut),
                          " level are orange.",
                          " MY4 points marked in pink."))
 p
}

brks <- c(seq(0, -30, -3) %>% rev, seq(3, 60, 3))

for(meas in mlist$MSR_RESULT_ID[1:5]){
  p <- f(meas)
  print(p)
  ggsave(here::here("results", paste0(meas, "_p", pcut, "_regression_facet.png")), plot=p, width=10, height=10, scale=1.25)
}

# look at PDI14RES for 52??? why not signif??
# results %>% filter(PPS_ID==52, MSR_RESULT_ID=="PDI14RES") %>% glimpse  # it is at 5%

```



## Facet plots for the 5 PPS's that had the greatest percentage of measures for which DSRIP was significant and indicated improvement
```{r pps_regplot, fig.height=12, fig.width=12}
# get the PPSs we want
plist <- gooddf %>%
  group_by(PPS_ID, PPS_NAME) %>%
  summarise(nregressions=n(), pct_signif=mean(good) * 100) %>%
  arrange(-pct_signif)

# we can't take all measures for each top-5 pps -- there are too many measures -- for now, just use the significant ones

g <- function(ppsid) {
  gname <- ppsdf %>% filter(PPS_ID == ppsid) %>% filter(row_number()==1) %>% .$idname
    
  p <- ppsdf %>% 
    filter(PPS_ID == ppsid) %>%
    left_join(gooddf %>% select(PPS_ID, MSR_RESULT_ID, good)) %>%
    filter(good==1) %>%
    mutate(my4=ifelse(PER_END_DT >= "2017-07-01" &  PER_END_DT <= "2018-03-31", 1, 0) %>% as.integer,
         good=factor(good),
         mname=paste0(MSR_RESULT_ID, ": ", msr_name(MSR_RESULT_ID)) %>% str_wrap(35)) %>%
    arrange(MSR_RESULT_ID, time) %>%
    ggplot(aes(time, MSR_RESULT)) +
    geom_line(colour="orange") +
    geom_point(colour="orange") +
    geom_line(aes(time, MSR_RESULT), colour="pink", data=. %>% filter(my4==1)) +
    geom_point(aes(time, MSR_RESULT), colour="pink", data=. %>% filter(my4==1)) +
    scale_x_continuous(breaks=brks) +
    geom_vline(xintercept = 1, colour="blue", linetype="solid") +
    geom_vline(xintercept = 12, colour="blue", linetype="dashed") +
    theme_bw() +
    facet_wrap(~mname, ncol=2, scales = "free") +
    theme(legend.position = "none") +
    theme(axis.text.x=element_text(angle=90, vjust=0.5)) +
    ggtitle(gname,
            subtitle=paste0("Selected measures with significant DSRIP effect (slope or intercept) at ", sprintf("%.2f", pcut),
                            " level. MY4 points marked in pink."))
 p
}

brks <- c(seq(0, -30, -3) %>% rev, seq(3, 60, 3))

for(ppsid in plist$PPS_ID[1:5]){
  p <- g(ppsid)
  print(p)
  ggsave(here::here("results", paste0("pps_", ppsid, "_regression_facet.png")), plot=p, width=10, height=10, scale=1.25)
}


```



# EXPERIMENTAL BELOW HERE
## outlier detection
```{r eval=FALSE}


```


## structural shifts detection
```{r eval=FALSE}


```


## outlier detection exploration
```{r eval=FALSE}
# library(DMwR)
# remove "Species", which is a categorical column
# iris2 <- iris[,1:4]
# outlier.scores <- lofactor(iris2, k=5)
# plot(density(outlier.scores))



dat.change <- c(12.013995263488, 11.8460207231808, 11.2845153487846, 11.7884417180764, 
11.6865425802022, 11.4703118125303, 11.4677576899063, 11.0227199625084, 
11.274775836817, 11.03073498338, 10.7771805591742, 10.7383206158923, 
10.5847230134625, 10.2479315651441, 10.4196381241735, 10.467607842288, 
10.3682422713283, 9.7834431752935, 9.76649842404295, 9.78257968297228, 
9.87817694914062, 9.3449034905713, 9.56400153361727, 9.78120084558148, 
9.3445162813738, 9.36767436354887, 9.12070987223648, 9.21909859069157, 
8.85136359917466, 8.8814423003979, 8.61830163359642, 8.44796977628488, 
8.06957847272046, 8.37999165387824, 7.98213210294954, 8.21977468333673, 
7.683960439316, 7.73213584532496, 7.98956476021092, 7.83036046746187, 
7.64496198988985, 4.49693528397253, 6.3459274845112, 5.86993447552116, 
4.58301192892403, 5.63419551523625, 6.67847511602895, 7.2005344054883, 
5.54970477623895, 6.00011922569104, 6.882667104467, 4.74057284230894, 
6.2140437333397, 6.18511450451019, 5.83973575417525, 6.57271194428385, 
5.36261938326723, 5.48948831338016, 4.93968645996861, 4.52598133247377, 
4.56372558828803, 5.74515428123725, 5.45931581984165, 5.58701112949141, 
6.00585679276365, 5.41639695946931, 4.55361875158434, 6.23720558202826, 
6.19433060301002, 5.82989415940829, 5.69321394985076, 5.53585871082265, 
5.42684812413063, 5.80887522466946, 5.56660158483312, 5.7284521523444, 
5.25425775891636, 5.4227645808924, 5.34778016248718, 5.07084809927736, 
5.324066161355, 5.03526881241705, 5.17387528516352, 5.29864121433813, 
5.36894461582415, 5.07436929444317, 4.80619983525015, 4.42858947882894, 
4.33623051506001, 4.33481791951228, 4.38041031792294, 3.90012900415342, 
4.04262777674943, 4.34383842876647, 4.36984816425014, 4.11641092254315, 
3.83985887104645, 3.81813419810962, 3.85174630901311, 3.66434598962311, 
3.4281724860426, 2.99726515704766, 2.96694634792395, 2.94003031547181, 
3.20892607367132, 3.03980832743458, 2.85952185077593, 2.70595278908964, 
2.50931109659839, 2.1912274016859)
dat.ts <- ts(dat.change, frequency=1)
data.ts.outliers <- tso(dat.ts)
data.ts.outliers
plot(data.ts.outliers)
str(data.ts.outliers)


# 22 PPRRES
tmp <- df1 %>%
  filter(PPS_ID==22, MSR_RESULT_ID=="PPRRES") %>% # 22 44
  mutate(rn=row_number())
plot(tmp$MSR_RESULT)


library(DMwR)
vars <- c("PPS_ID", "PPS_NAME", "rn", "PER_END_DT", "time", "MSR_RESULT_ID", "MSR_RESULT")

kval <- 5
t2 <- df1 %>%
  filter(PPS_ID %in% c(22, 43, 44), MSR_RESULT_ID=="PPRRES") %>% # 22 44
  arrange(MSR_RESULT_ID, PPS_ID, PPS_NAME, PER_END_DT) %>%
  group_by(MSR_RESULT_ID, PPS_ID, PPS_NAME) %>%
  mutate(rn=row_number()) %>%
  select(vars) %>%
  mutate(change_back=ifelse(rn==min(rn), 0, MSR_RESULT - lag(MSR_RESULT)),
         change_fwd=ifelse(rn==max(rn), 0, lead(MSR_RESULT) - MSR_RESULT),
         lofb=lofactor(change_back, k=kval),
         loff=lofactor(change_fwd, k=kval),
         outlier=(lofb > 2 & loff > 2)) %>%
  time_decompose(MSR_RESULT, merge = TRUE) %>%
  anomalize(remainder) %>%
  time_recompose() %>%
  ungroup
t2
t2 %>% filter(PPS_ID==44)

(brks <- c(seq(0, -30, -3) %>% rev, seq(3, 60, 3)))
lofcut <- 1.75
p <- t2 %>%
  ggplot(aes(time, MSR_RESULT)) +
  geom_line() +
  geom_point() +
  # geom_point(aes(time, MSR_RESULT), colour="red", size=1.5, data=. %>% filter(outlier == TRUE)) +
  geom_point(aes(time, MSR_RESULT), colour="blue", size=2, data=. %>% 
               filter((abs(lofb) > lofcut & abs(loff) > lofcut) & (anomaly != "Yes"))) +
  geom_point(aes(time, MSR_RESULT), colour="yellow", size=2, data=. %>% 
               filter(!(abs(lofb) > lofcut & abs(loff) > lofcut) & (anomaly == "Yes"))) +
  geom_point(aes(time, MSR_RESULT), colour="green", size=3, data=. %>% 
               filter((abs(lofb) > lofcut & abs(loff) > lofcut) & (anomaly == "Yes"))) +
  scale_x_continuous(breaks=brks) +
  geom_vline(xintercept = 1, colour="blue", linetype="solid") +
  geom_vline(xintercept = 12, colour="blue", linetype="dashed") +
  theme_bw() +
  facet_wrap(~PPS_ID + PPS_NAME, ncol=5, scales = "free")
p

     
vars <- c("PPS_ID", "PPS_NAME", "rn", "PER_END_DT", "time", "MSR_RESULT_ID", "MSR_RESULT")
tmp2 <- tmp %>%
  select(vars) %>%
  mutate(change=MSR_RESULT - lag(MSR_RESULT),
         change=ifelse(rn==1, 0, change),
         change_ratio= change / sd(change),
         level_ratio=(MSR_RESULT - mean(MSR_RESULT)) / sd(MSR_RESULT))

t2 <- ts(tmp$MSR_RESULT, frequency=1)
system.time(t2o <- tso(t2, logfile="tso_auto_out.out",))
t2o
str(t2o)
t2o$outliers %>% as_tibble()


system.time(t2a <- tso(t2, 
                       types = c("TC"), # c("AO", "LS", "TC", "IO"),
                       cval = 3.5, # lower is more sensitive to AO outliers
                       delta = 0.1,
                       maxit = 1,
                       maxit.iloop = 6, maxit.oloop = 6,
                       # logfile="tso_out.out",
                       tsmethod="arima", 
                       args.tsmethod = list(order=c(1, 1, 0))))


t2a <- tso(t2, types = c("TC"), # c("AO", "LS", "TC"),
           discard.method = "bottom-up", tsmethod = "auto.arima",
           args.tsmethod = list(allowdrift = TRUE, ic = "bic"))


t2a
outdf <- t2a$outliers %>% as_tibble() %>% rename(rn=time) %>% left_join(tmp) %>% select(rn, type, coefhat, MSR_RESULT)
tmp %>%
  ggplot(aes(rn, MSR_RESULT)) +
  geom_line() +
  geom_point() +
  geom_point(aes(rn, MSR_RESULT), colour="red", size=3, data=outdf %>% filter(type=="AO")) +
  geom_point(aes(rn, MSR_RESULT), colour="green", size=3, data=outdf %>% filter(type=="LS")) +
  geom_point(aes(rn, MSR_RESULT), colour="blue", size=3, data=outdf %>% filter(type=="TC")) +
  geom_point(aes(rn, MSR_RESULT), colour="orange", size=3, data=outdf %>% filter(type=="IO")) +
  scale_x_continuous(breaks=seq(0, 60, 5)) +
  theme_bw()



t2a <- tmp %>% 
  select(rn, PER_END_DT, MSR_RESULT) %>%
  time_decompose(MSR_RESULT, merge = TRUE) %>%
  anomalize(remainder) %>%
  time_recompose()
outdf <- t2a$outliers %>% as_tibble() %>% rename(rn=time) %>% left_join(tmp) %>% select(rn, type, coefhat, MSR_RESULT)
t2a %>%
  ggplot(aes(rn, MSR_RESULT)) +
  geom_line() +
  geom_point() +
  geom_point(aes(rn, MSR_RESULT), colour="red", size=3, data=. %>% filter(anomaly == "Yes")) +
  scale_x_continuous(breaks=seq(0, 60, 5)) +
  theme_bw()


tmp2 <- df1 %>%
  filter(MSR_RESULT_ID=="PPRRES") %>%
  select(PPS_ID, PPS_NAME, PER_END_DT, time, MSR_RESULT) %>%
  group_by(PPS_ID, PPS_NAME) %>%
  arrange(PER_END_DT) %>%
  time_decompose(MSR_RESULT, merge = TRUE) %>%
  anomalize(remainder,
            alpha = .025, # .05 default, smaller makes it harder to be an outlier
            max_anoms = .05, # 0.2 default maximum proportion of obs that can be an anomaly
            ) %>%
  time_recompose() %>%
  ungroup
(brks <- c(seq(0, -30, -3) %>% rev, seq(3, 60, 3)))
p <- tmp2 %>%
  ggplot(aes(time, MSR_RESULT)) +
  geom_line() +
  geom_point() +
  geom_point(aes(time, MSR_RESULT), colour="red", size=1.5, data=. %>% filter(anomaly == "Yes")) +
  scale_x_continuous(breaks=brks) +
  geom_vline(xintercept = 1, colour="blue", linetype="solid") +
  geom_vline(xintercept = 12, colour="blue", linetype="dashed") +
  theme_bw() +
  facet_wrap(~PPS_ID + PPS_NAME, ncol=5, scales = "free")
p

library(strucchange)
data("Nile")
plot(Nile)

bp.nile <- breakpoints(Nile ~ 1)
summary(bp.nile)
plot(bp.nile)

## compute breakdates corresponding to the
## breakpoints of minimum BIC segmentation
breakdates(bp.nile)

## confidence intervals
ci.nile <- confint(bp.nile)
breakdates(ci.nile)
ci.nile

plot(Nile)
lines(ci.nile)


d1 <- df1 %>%
  filter(PPS_ID==23, MSR_RESULT_ID=="PPRRES") %>% # 22 44 43
  mutate(rn=row_number())
d1 %>% ggplot(aes(rn, MSR_RESULT)) + geom_line() + geom_point()
bp <- breakpoints(d1$MSR_RESULT ~ 1, breaks=5)
str(bp)
d1 %>% ggplot(aes(rn, MSR_RESULT)) + 
  geom_line() + 
  geom_point() +
  geom_vline(xintercept = bp$breakpoints)



resNile2 <- tso(y = Nile, types = c("AO", "LS", "TC"),
                discard.method = "bottom-up", tsmethod = "auto.arima",
                args.tsmethod = list(allowdrift = FALSE, ic = "bic"))
resNile2
Nile



system.time(t2a <- tso(t2, 
                       tsmethod="arima", 
                       args.tsmethod = list(order=c(0, 0, 11),
                       fixed = c(rep(1, 11), NA))
                       ))
t2a

arima(t2, order=c(0, 0, 11), fixed = c(rep(1, 11), NA))
summary(arima(t2, order=c(1, 1, 0)))

    # fit = map(data, ~ arima(.$MSR_RESULT, 
    #                         xreg = as.matrix(.[, c("time_ma", "dsrip_ma", "interaction_ma")]),
    #                         order=c(0, 0, 11),
    #                         fixed = c(rep(1, 11), NA, NA, NA, NA ))),


tmp %>%
  ggplot(aes(rn, MSR_RESULT)) +
  geom_line() +
  geom_point() +
  geom_point(aes(rn, MSR_RESULT), colour="red", size=2, data=tmp %>% filter(rn %in% c(44, 45, 49))) +
  scale_x_continuous(breaks=seq(0, 60, 5))

tmp2 %>%
  ggplot(aes(time, MSR_RESULT)) +
  geom_line() +
  geom_point() +
  geom_point(aes(time, MSR_RESULT), colour="red", size=2, data=tmp2 %>% filter(change_ratio > 2 | level_ratio > 2))

# https://towardsdatascience.com/tidy-anomaly-detection-using-r-82a0c776d523
library("anomalize")
library(coindeskr) #bitcoin price extraction from coindesk
btc <- get_historic_price(start = "2017-01-01")
btc_ts <- btc %>% rownames_to_column() %>% as.tibble() %>% 
  mutate(date = as.Date(rowname)) %>% select(-one_of('rowname'))
ht(btc_ts)

t3 <- btc_ts %>% 
  time_decompose(Price, method = "stl", frequency = "auto", trend = "auto") %>%
  anomalize(remainder, method = "gesd", alpha = 0.05, max_anoms = 0.2)
ht(t3)

t3 %>%
  plot_anomaly_decomposition()

t3 <- tmp %>% 
  time_decompose(MSR_RESULT, method = "stl", frequency = "auto", trend = "auto") %>%
  anomalize(remainder, method = "gesd", alpha = 0.05, max_anoms = 0.2)
ht(t3)

t3 %>%
  plot_anomaly_decomposition()

t3 %>%
  left_join(tmp) %>%
  ggplot(aes(time, MSR_RESULT)) +
  geom_line() +
  geom_point() +
  geom_point(aes(time, MSR_RESULT), colour="red", size=2, data=. %>% filter(anomaly=="Yes"))


```



## explore
```{r eval=FALSE}
msrid <- "SMCRES"
msrid <- "SAARES"
msrid <- "PPRRES"

(msrtitle <- paste0(msrid, ": ", ulabs$MSR_RESULT_NAME[ulabs$MSR_RESULT_ID==msrid]))
(msrcalc <- ulabs$UNIT_LBL[ulabs$MSR_RESULT_ID==msrid])
p <- ppsall %>%
  filter(MSR_RESULT_ID==msrid) %>%
  ggplot(aes(PER_END_DT, MSR_RESULT)) +
  geom_line(colour="darkgreen") +
  geom_point(colour="darkgreen") +
  geom_vline(xintercept=dsrip_start, linetype="dashed", colour="blue", size=1) +
  facet_wrap(~idname, scales = "free", ncol=6) +
  ggtitle(paste0(msrtitle, "\n", paste0("Calculation: ", msrcalc)),
          subtitle="Notes: (1) Vertical line marks DSRIP start, (2) Data are from Chris dropbox files")
p
ggsave(here::here("results", paste0(msrid, "_facet.png")), plot=p, width=10, height=8, scale=2)
  
# produce a bunch of plots
# measure_facet_plots
f <- function(msrid){
  # CAUTION: this function assumes ulabs and ppsall data frames exist
  msrtitle <- paste0(msrid, ": ", ulabs$MSR_RESULT_NAME[ulabs$MSR_RESULT_ID==msrid])
  msrcalc <- ulabs$UNIT_LBL[ulabs$MSR_RESULT_ID==msrid]
  p <- ppsall %>%
    filter(MSR_RESULT_ID==msrid) %>%
    ggplot(aes(PER_END_DT, MSR_RESULT)) +
    geom_line(colour="darkgreen") +
    geom_point(colour="darkgreen") +
    geom_vline(xintercept=dsrip_start, linetype="dashed", colour="blue", size=1) +
    facet_wrap(~idname, scales = "free", ncol=6) +
    ggtitle(paste0(msrtitle, "\n", paste0("Calculation: ", msrcalc)),
          subtitle="Notes: (1) Vertical line marks DSRIP start, (2) Data are from Chris dropbox files")
  ggsave(here::here("results", "measure_facet_plots", paste0(msrid, "_facet.png")), plot=p, width=10, height=8, scale=2)
  return(NULL)
}

l_ply(ulabs$MSR_RESULT_ID, f, .progress = "text")

```


